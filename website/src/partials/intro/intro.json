{
	"title": "Introduction",
	"paragraphs": [
		{
			"text": "To allocate funding and target educational support appropriately, we need to accurately measure how well our children perform academically. To assess general writing ability, written essays are an invaluable tool. Unfortunately, grading millions of essays across the United States is costly and time-consuming for teachers. As a result, school systems turn to easier-to-grade alternatives, like multiple choice tests."
		},
		{
			"text": "One solution to this problem is automatic essay grading. Automatic essay grading uses prior data on written essays (i.e. grades assigned by human judges) to predict the grades of future essays. This saves a significant amount of time and educational resources and helps make essays a more feasible part of standardized testing.",
			"image": "../../graphs/IntroGraphic.png",
			"width": "500px"
		},
		{
			"text": "DuQuesne University <a href='http://www.duq.edu/about/centers-and-institutes/center-for-teaching-excellence/teaching-and-learning/strengths-and-dangers-of-essay-questions-for-exams'>notes</a> that teachers generally grade essays using one of two methods: a <span class='bold'>holistic approach</span> or an <span class='bold'>analytical approach</span>. A holistic grading approach involves reading all responses to an essay question and then assigning grades using a measure of relative quality. The analytical approach, on the other hand, is more systematic: this involves making a list of the expected qualities (a.k.a. features) of a good essay response, and then evaluating essay responses against that list."
		},
			{
			"text": "Because the analytical approach to essay scoring is more systematic and involves a more intuitive mapping onto statistical modeling, we've decided to a build a model which scores essays analytically instead of holistically. We describe the intricacies of our model later on this website."
		},
		{
			"text": "For this project, we looked at roughly 12,000 essays from 8 separate standardized tests, each essay written by students between grades 7 and 10. These written assessments were responses to open ended questions posed by an instructor (e.g. “When have you shown patience?”). Each of the essays was graded by a human on a numerical scale specific to the essay prompt. In other words, some essays were graded on a scale from 0 to 5, and others from 0 to, say, 10. These essays were cleaned of personal identifying information using the Named Entity Recognizer from the Stanford NLP group, both for anonymity and because of a potential for bias against certain names."
		}
	]
}
