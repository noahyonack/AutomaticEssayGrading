<html lang="en">
    
    <head>
        <meta charset="UTF-8">
        <title>CS109a</title>
        <link rel="stylesheet" href="main.css">
    </head>
    
    <body>
        <header class="header" id="header">
            	<h1 class="name">Automatic Essay Grading</h1> 
            <nav class="nav">
                <ul class="nav-list">
                    <li class="nav-list-item"><a href="#" id=home-link class="nav-list-item-link selected">Home</a>
                    </li>
                    <li class="nav-list-item"><a href="#" id=intro-link class="nav-list-item-link ">Intro</a>
                    </li>
                    <li class="nav-list-item"><a href="#" id=data-link class="nav-list-item-link ">Data</a>
                    </li>
                    <li class="nav-list-item"><a href="#" id=exploration-link class="nav-list-item-link ">Exploration</a>
                    </li>
                    <li class="nav-list-item"><a href="#" id=model-link class="nav-list-item-link ">Model</a>
                    </li>
                    <li class="nav-list-item"><a href="#" id=future-link class="nav-list-item-link ">Future</a>
                    </li>
                </ul>
            </nav>
        </header>
        <section class="home" id="home">
            <article class="home-browsers">
                <div class="sublime">
                    <img src="graphs/home_notebook.png" />
                </div>
                <div class="terminal">
                    <article class="browser">
                        <article class="browser-header">
                            <div class="button-container">
                                <div class="close-button"></div>
                                <div class="minimize-button"></div>
                                <div class="fullsize-button"></div>
                            </div>
                        </article>
                        <p class="terminal-text">PavlosOS:Desktop ~ $ cd /DataScience</p>
                        <p class="terminal-text">PavlosOS:DataScience ~ $ pwd</p>
                        <p class="terminal-text">/Users/Stephen&amp;Noah/DataScience</p>
                        <p class="terminal-text">PavlosOS:DataScience ~ $ ls</p>
                        <div class="terminal-column">
                            <p class="terminal-text">models.ipynb</p>
                            <p class="terminal-text">training.csv</p>
                        </div>
                        <div class="terminal-column">
                            <p class="terminal-text">testing.csv</p>
                            <p class="terminal-text">pavlos.py</p>
                        </div>
                        <p class="terminal-text">PavlosOS:DataScience ~ $ jupyter notebook</p>
                        <p class="terminal-text">&gt; Serving notebooks from local directory</p>
                    </article>
                </div>
            </article>
            <article class="title">
                	<h2 class="headline">Automatic Essay Grading</h2>

                	<h3 class="subheadline">Stephen Turban &amp; Noah Yonack, Harvard College (CS109a/Stat121a)</h2>
    		</article>
    		<i class="down-arrow"></i>
    </section>
    <section class="intro" id="intro">
    		<div class="container">
    			<h2 class="title">Introduction</h2>
    				<p class="explanation">To allocate funding and target educational support appropriately, we need to accurately measure how well our children perform academically. To assess general writing ability, written essays are an invaluable tool. Unfortunately, grading millions of essays across the United States is costly and time-consuming for teachers. As a result, school systems turn to easier-to-grade alternatives, like multiple choice tests.</p>
    				<p class="explanation">One solution to this problem is automatic essay grading. Automatic essay grading uses prior data on written essays (i.e. grades assigned by human judges) to predict the grades of future essays. This saves a significant amount of time and educational resources and helps make essays a more feasible part of standardized testing.</p>
    					<img class="illustration" width=500px src=graphs/IntroGraphic.png />
    				<p class="explanation">DuQuesne University <a href='http://www.duq.edu/about/centers-and-institutes/center-for-teaching-excellence/teaching-and-learning/strengths-and-dangers-of-essay-questions-for-exams'>notes</a> that teachers generally grade essays using one of two methods: a <span class='bold'>holistic approach</span> or an <span class='bold'>analytical approach</span>. A holistic grading approach involves reading all responses to an essay question and then assigning grades using a measure of relative quality. The analytical approach, on the other hand, is more systematic: this involves making a list of the expected qualities (a.k.a. features) of a good essay response, and then evaluating essay responses against that list.</p>
    				<p class="explanation">Because the analytical approach to essay scoring is more systematic and involves a more intuitive mapping onto statistical modeling, we'v decided to a build a model which scores essays analytically instead of holistically. We describe the intricacies of our model later on this website.</p>
    				<p class="explanation">For this project, we looked at roughly 12,000 essays from 8 separate standardized tests, each written by students between grade 7 and 10. These written assessments were responses to open ended questions posed by an instructor (e.g. “When have you shown patience?”). Each of the essays was graded by a human on a numerical scale specific to the essay prompt. In other words, some essays were graded on a scale from 0 to 5, and others from 0 to, say, 10. These essays were cleaned of personal identifying information both for anonymity and a potential for bias against certain names using the Named Entity Recognizer from the Stanford NLP group.</p>
    		</div>
    </section>
    <section class="data" id="data">
    		<div class="container">
    			<h2 class="title">Data</h2>
    				<p class="explanation">We downloaded our data from a Kaggle <a href='https://www.kaggle.com/c/asap-aes/data'>competition</a> hosted by The Hewlett Foundation. We use this section to describe the structure of the data (i.e. the data dictionary), and we perform preliminary exploration of this data in the next section (see <a href='#exploration'>Data Exploration</a> below).</p>
    				<p class="explanation">The data is comprised of 5 main columns (there are a handful of other columns as well, but they are only partially complete and are less important than the ones listed here): <span class='bold'>essay_set, essay, rater1_domain1, rater2_domain1, and domain1_score</span>.</p>
    				<p class="explanation"><ul class='column-list'><li>The <span class='bold'>essay_set</span> column contains a number between 1 and 8 which indicates the prompt for the given essay — because we know that essay sets don't necessarilly have the same grading rubric, it seems clear that this column will be valuable as a feature in our model.</li><li>The <span class='bold'>essay</span> column contains the raw essay text (which, as mentioned in the previous section) has been cleaned of personal identifying information. Immediately, we know that this column can't be used as a model feature without some sort of preprocessing. In the sections below, we describe our preprocessing method, known as tf-idf vectorization.</li><li>The <span class='bold'>rater1_domain1</span> and <span class='bold'>rater2_domain1</span> columns represent grades for the given essay by two independent expert raters, both human. It is common practice two involve two independent human graders in the task of grading essays from standardized tests, as a means of reducing bias and error. The <span class='bold'>domain1_score</span> consolidates the scores from both judges by adding the two. We treat this composite number as our response variable.</li></ul></p>
    					<img class="illustration" width=900px src=graphs/essay_df_sample.png />
    		</div>
    </section>
    <section class="exploration" id="exploration">
    		<div class="container">
    			<h2 class="title">Data Exploration</h2>
    				<p class="explanation">As mentioned, our essays come from 8 different sets, each of which has a different score range. As you can see from the histogram below, each essay set has roughly the same prevalence within the dataset, except for essay set 8.</p>
    					<img class="illustration" width=500px src=graphs/essay_set_distribution.png />
    				<p class="explanation">We began the exploration phase of the data-science process by looking at fundamental properties of the dataset: histograms of essay scores, scatterplots between essay length and score, and scatterplots between the number of unique words per essay and score. Finding a relationship between these basic features and our response variable could mean better predictions when we build our model later down the road.</p>
    				<p class="explanation">We first plotted a histogram of all essay scores. While some essay sets are graded on scales from, say, 0 - 5, others have scales that go up to 30 (and, when a student gets a score of 20 from both independent graders, her domain1_score is 60). Notice in the histogram below the multiple local modes — one at ~4, ~9, ~40. This is because we are plotting the distribution of all scores, regardless of essay set. Because we're seeing multiple 'smaller' distributions on the same axis, we know that including <span class='bold'>essay_set</span> in our model will likely help us predict score by limiting the range of our predicted response.</p>
    					<img class="illustration" width=500px src=graphs/essay_score_hist.png />
    				<p class="explanation">We also looked at the relationship between essay length and our response variable. Notice the multiple `fingers` in the scatterplot below, indicating at least moderate-strength correlations between essay length and score.</p>
    					<img class="illustration" width=500px src=graphs/essay_length_score_scatter.png />
    				<p class="explanation">Breaking up this scatterplot into its constituent parts (by essay set), these correlations become more obvious. For instance, notice the relationship between essay length vs. score for essay set 1. These two variables have a Spearman's rho correlation coefficient of 0.80, which is considered to be strong. Other essay sets have similar scatterplots and correlation coefficients.</p>
    					<img class="illustration" width=500px src=graphs/essay_length_score_scatter_1.png />
    				<p class="explanation">Lastly, we also looked at essay score as a function of the number of unique words in that essay. Unsurprisingly, essay length is an almost perfect proxy of the number of unique words per essay, meaning the correlation between the two variables is almost 1. Thus, using both essay length and the number of unique words as features in our model is redundant.</p>
    		</div>
    </section>
    <section class="model" id="model">
    		<div class="container">
    			<h2 class="title">Model</h2>
    				<p class="explanation">Lorem ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum</p>
    					<img class="illustration" src=../assets/img/self-shot-1.jpg />
    				<p class="explanation">Lorem ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum</p>
    				<p class="explanation">Lorem ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum ipsum</p>
    		</div>
    </section>
    <section class="future" id="future">
    		<div class="container">
    			<h2 class="title">Future Work</h2>
    				<p class="explanation">There are many avenues for future work. For one, a more sophisticated model might consider the implications of <span class='bold'>biased training data</span>. One of the greatest advantages of grading essays using a computer in lieu of a human is that computers aren't subject to same psychological biases that humans are — or are they? When we train our l1-regularized linear model on essays graded by humans, we "learn" the biases inherent in the grading system.</p>
    				<p class="explanation">What kinds of biases are we talking about? There are many: research <a href='https://etd.lib.msu.edu/islandora/object/etd%3A1970/datastream/OBJ/view'>suggests</a> that typed essays tend to get better grades than handwritten essays, even when the text is the exact same for both! Similarly, human judgement is riddled with biases: we know that judges <a href='http://www.pnas.org/content/108/17/6889'>give more lenient sentences</a> to criminals in the morning and early afternoon — right after breakfast and lunch, respectively. When teachers grade students' essays, they naturally encode their biases directly into the grade, albeit unintentionally. How can we correct for biases like this?</p>
    					<img class="illustration" src=graphs/no_bias.png />
    				<p class="explanation">Without changing the way that teachers grade, we'll have trouble getting our hands on unbiased data. On the other hand, we may be able to prejudice our model using <span class='bold'>priors</span>. If we know that a certain bias is pervasive and that it causes grades to go up, for instance, we can include a prior in our model which controls for this unnencessary shift up. We may have to think intuitively about how a certain bias might manifest itself, or we can collect more granular data (e.g. the time of day at which the essay was graded) to do more rigorous analysis of the effects of bias.</p>
    				<p class="explanation">Another improvement to our model might involve pulling out <span class='bold'>more sophisticated features</span>. For instance, one property of an essay which is likely to be valuable is its gramaticallity. There are certainly many ways to calculate an essay's grammaticallity (for instance, see <a target='_blank' href='graphs/paper.pdf'>this</a> paper by the ETS, which uses ridge regression). One simpler approach might involve calculating the proportion of mispelled words (by using a dictionary, for instance). Either way, pulling out grammaticallity would surely help improve the accuracy of our model.</p>
    				<p class="explanation">Lastly, we might also consider swapping our regression model for a <span class='bold'>neural net</span>. There are many <a href='https://da352.user.srcf.net/public_uploads/acl2016.pdf'>examples of models</a> which have used Long-Short Term Memory networks to solve the problem of automatic essay scoring. <span class='bold'>That said, even the most sophisticated approaches involving neural networks seem to yield accuracies that are no better than our regularized LASSO regression.</span></p>
    		</div>
    </section>
    
    <script src="all.min.js"></script>
      <script src="http://localhost:8080/livereload.js"></script>
  </body>
</html>